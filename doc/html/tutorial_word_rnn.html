<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-->
<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"></meta>
<meta http-equiv="X-UA-Compatible" content="IE=9"></meta>
<title>Snapdragon Neural Processing Engine SDK: Running the Word-RNN Model</title>
<link href="tabs.css" rel="stylesheet" type="text/css"></link>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="autoEnterCurrentDate.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="is.css" rel="stylesheet" type="text/css" ></link>
<link href="custom.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Snapdragon Neural Processing Engine SDK
   <span id="projectnumber"></span></div>
   <div id="projectbrief">Reference Guide</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('tutorial_word_rnn.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Running the Word-RNN Model </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="tutorial_word_rnn_table_of_content"></a>
Table Of Contents</h1>
<ul>
<li><a class="el" href="tutorial_word_rnn.html#tutorial_word_rnn_overview_native_app">Overview</a></li>
<li><a class="el" href="tutorial_word_rnn.html#tutorial_word_rnn_overview">Introduction</a></li>
<li><a class="el" href="tutorial_word_rnn.html#tutorial_word_rnn_prerequisites">Prerequisites</a></li>
<li><a class="el" href="tutorial_word_rnn.html#tutorial_word_rnn_create_train_convert">Create, Train, and Convert Word-RNN Model</a></li>
<li><a class="el" href="tutorial_word_rnn.html#tutorial_word_rnn_run_on_linux_host">Run on Linux Host</a></li>
<li><a class="el" href="tutorial_word_rnn.html#tutorial_word_rnn_run_on_android">Run on Android Target</a></li>
<li><a class="el" href="tutorial_word_rnn.html#tutorial_word_rnn_run_on_android_cpu">Running on Android using CPU Runtime</a></li>
</ul>
<h1><a class="anchor" id="tutorial_word_rnn_overview_native_app"></a>
Overview</h1>
<p>The example C++ application in this tutorial is called <b>snpe-net-run</b>. It is a command line executable that executes a neural network using SNPE SDK APIs.</p>
<p>The required arguments to snpe-net-run are:</p><ul>
<li>A neural network model in the DLC file format</li>
<li>An input list file with paths to the input data.</li>
</ul>
<p>Optional arguments to snpe-net-run are:</p><ul>
<li>Choice of GPU or DSP runtime (default is CPU)</li>
<li>Output directory (default is ./output)</li>
<li>Show help description</li>
</ul>
<p>snpe-net-run creates and populates an output directory with the results of executing the neural network on the input data.</p>
<p><a class="anchor" id="fig_neural_network"></a>  <div style= text-align:left;><img src="images/neural_network.png" alt="SNPE" width="40%" height="40%"><br/><br/><b></b></div><br/> </p>
<p>The SNPE SDK provides Linux and Android binaries of <b>snpe-net-run</b> under</p><ul>
<li>$SNPE_ROOT/bin/x86_64-linux-clang</li>
<li>$SNPE_ROOT/bin/arm-android-clang6.0</li>
<li>$SNPE_ROOT/bin/aarch64-android-clang6.0</li>
<li>$SNPE_ROOT/bin/aarch64-oe-linux-gcc6.4</li>
<li>$SNPE_ROOT/bin/arm-oe-linux-gcc6.4hf</li>
</ul>
<h1><a class="anchor" id="tutorial_word_rnn_overview"></a>
Introduction</h1>
<p>Recurrent Neural Network (RNN) architectures are widely used in Machine Learning Applications for processing sequential input data. This chapter will show a simple Word-RNN example for predicting the next word in an embedding using Long Short-Term Memory (LSTM). The step-by-step example will create, train, convert, and execute a Word-RNN model with SNPE.</p>
<p>The external python3 packages needed by this example are:</p><ul>
<li>numpy</li>
<li>pandas</li>
<li>sklearn</li>
<li>tensorflow (1.6 or 1.11)</li>
</ul>
<p>There are six files in $SNPE_ROOT/models/word_rnn folder</p><ul>
<li>inference.py</li>
<li>input_list.txt</li>
<li>belling_the_cat.txt</li>
<li>word_rnn.py</li>
<li>word_rnn_adb.sh</li>
<li>NOTICE.txt</li>
</ul>
<p>The <b>word_rnn.py</b> python3 script creates and trains an RNN model with one LSTM layer. After RNN training is done, the corresponding frozen protobuf file will be generated.</p>
<p>The <b>inference.py</b> prompts the user to enter several words, at which point <b>snpe-net-run</b> will be called in a loop to generate subsequent words.</p>
<h1><a class="anchor" id="tutorial_word_rnn_prerequisites"></a>
Prerequisites</h1>
<ul>
<li>
The SNPE SDK has been set up following the <a class="el" href="setup.html">SNPE Setup</a> chapter. </li>
<li>
The <a class="el" href="tutorial_setup.html">Tutorials Setup</a> has been completed. </li>
<li>
TensorFlow is installed (see <a class="el" href="setup_tensorflow.html">TensorFlow Setup</a>) </li>
</ul>
<h1><a class="anchor" id="tutorial_word_rnn_create_train_convert"></a>
Create, Train, and Convert Word-RNN Model</h1>
<p>Run word_rnn.py to create and train the Word-RNN model.</p>
<pre class="fragment">cd $SNPE_ROOT/models/word_rnn
python3 word_rnn.py
</pre><p>The terminal will show following messages.</p>
<pre class="fragment">Training will be logged in word_rnn_log.
Load training file belling_the_cat.txt.
Embedding created.
Iter= 1000
Iter= 2000
Iter= 3000
Iter= 4000
Iter= 5000
Optimization done.
Save frozen graph in word_rnn.pb.
</pre><p>Then, convert the frozen graph model with snpe-tensorflow-to-dlc.</p>
<pre class="fragment">snpe-tensorflow-to-dlc --input_network word_rnn.pb \
                       --input_dim Placeholder "1, 4, 1" \
                       --out_node "rnn/basic_lstm_cell/Mul_11" \
                       --output_path word_rnn.dlc
</pre><p>After dlc conversion, we can view the converted dlc architecture with <b>snpe-dlc-info</b> and <b>snpe-dlc-viewer</b> as follows:</p>
<pre class="fragment">snpe-dlc-info -i word_rnn.dlc
</pre><p>The output will be:</p>
<pre class="fragment">-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| Id | Name                              | Type    | Inputs                            | Outputs                                     | Out Dims | Runtimes | Parameters                       |
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| 0  | Placeholder:0                     | data    | Placeholder:0                     | Placeholder:0                               | 1x4x1    | A D G C  | input_preprocessing: passthrough |
|    |                                   |         |                                   |                                             |          |          | input_type: default              |
| 1  | Reshape:0                         | reshape | Placeholder:0                     | Reshape:0                                   | 1x4      | A D G C  |                                  |
| 2  | rnn/basic_lstm_cell/Mul_2_reshape | reshape | Reshape:0                         | rnn/basic_lstm_cell/Mul_2_reshape           | 1x4x1    | A D G C  |                                  |
| 3  | rnn/basic_lstm_cell/Mul_11:0      | lstm    | rnn/basic_lstm_cell/Mul_2_reshape | rnn/basic_lstm_cell/Mul_11:0_all_time_steps | 1x4x112  | A     C  | x weights: (1, 448)              |
|    |                                   |         |                                   |                                             |          |          | h weights: (112, 448)            |
|    |                                   |         |                                   |                                             |          |          | biases: (448,)                   |
|    |                                   |         |                                   |                                             |          |          | backward: False                  |
|    |                                   |         |                                   |                                             |          |          | param count: 51k (100%)          |
|    |                                   |         |                                   |                                             |          |          | MACs per inference: 203k (100%)  |
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
</pre><pre class="fragment">snpe-dlc-viewer -i word_rnn.dlc
</pre><p>The output network model HTML file will be saved at /tmp/word_rnn.html.</p>
<h1><a class="anchor" id="tutorial_word_rnn_run_on_linux_host"></a>
Run on Linux Host</h1>
<p>Go to the base location for the model and run the python3 script including <b>snpe-net-run</b></p>
<pre class="fragment">python3 inference.py
</pre><p>After running <b>inference.py</b>, you will see a list of word embedding keys and user input prompt as follows:</p>
<pre class="fragment">Load training file belling_the_cat.txt.
Embedding created.
Use host cpu.
Display word embedding keys:
dict_keys(['long', 'is', 'up', 'it', 'i', 'chief', 'our', 'procured', 'her', 'in', 'mouse', 'council', 'treacherous', 'meet', 'manner', 'approaches', 'with', 'propose', 'which', 'consider', 'thought', 'know', 'bell', 'signal', 'always', 'by', 'small', 'old', 'could', 'about', 'neck', 'of', 'approach', 'well', 'easy', 'take', 'all', 'outwit', 'met', 'they', 'this', 'who', 'cat', 'what', '.', 'will', 'attached', 'their', 'when', 'receive', 'agree', 'applause', 'and', 'if', 'now', 'to', 'a', 'round', 'enemy', 'was', 'ribbon', 'us', 'had', 'general', 'ago', 'means', 'last', 'venture', 'got', 'sly', 'measures', 'young', 'she', 'very', 'impossible', 'therefore', 'we', 'should', 'one', 'mice', 'case', '?', 'make', 'nobody', 'he', 'that', 'consists', 'spoke', 'from', 'easily', 'at', 'neighbourhood', 'the', 'looked', 'then', 'until', 'an', 'common', 'but', 'be', 'would', 'danger', 'retire', 'proposal', 'another', 'you', ',', 'while', 'escape', 'some', 'remedies', 'said'])
Please input 4 words:
</pre><p>User can input embedded words and see the results. For example: <b>long ago , the</b></p>
<pre class="fragment">...
...
-------------------------------------------------------------------------------
Model String: N/A
SNPE vX.Y.Z.dev
-------------------------------------------------------------------------------
Processing DNN input(s):
./input.raw
-------------------------------------------------------------------------------
Model String: N/A
SNPE vX.Y.Z.dev
-------------------------------------------------------------------------------
Processing DNN input(s):
./input.raw
Inference result: long ago , the said she a , , and the could could , , and the could could , , and the could could , , and the could could , , and the could
</pre><p>The <b>inference.py</b> will call <b>snpe-net-run</b> several times to generate subsequent words with trained LSTM model.</p>
<p><b>Binary data input</b></p>
<p>Note that the Word-RNN model does not accept pure text files as input. The model expects its input tensor dimension to be <b>1x4x1</b> as a float array.</p>
<p>The <b>create_embedding</b> function in <b>inference.py</b> will parse, collect, encode, and build the word embedding.</p>
<p>User inputs will then be transformed into a <b>1x4x1</b> vector and sent into the LSTM model. Afterwards the LSTM output will be also transformed into the corresponding embedded word.</p>
<h1><a class="anchor" id="tutorial_word_rnn_run_on_android"></a>
Run on Android Target</h1>
<p><b>Select target architecture</b></p>
<p>SNPE provides Android binaries for armeabi-v7a and arm64-v8a architectures. For each architecture, the binaries are compiled with clang6.0 using libc++ STL implementation. The following shows the commands to select the desired binaries.</p>
<pre class="fragment"># architecture: armeabi-v7a - compiler: clang - STL: libc++
export SNPE_TARGET_ARCH=arm-android-clang6.0
export SNPE_TARGET_STL=libc++_shared.so

# architecture: arm64-v8a - compiler: clang - STL: libc++
export SNPE_TARGET_ARCH=aarch64-android-clang6.0
export SNPE_TARGET_STL=libc++_shared.so
</pre><p>For simplicity, this tutorial sets the target binaries to arm-android-clang6.0, which uses libc++_shared.so, for commands on host and on target.</p>
<p><b>Push binaries to target</b></p>
<p>Push SNPE libraries and the prebuilt snpe-net-run executable to /data/local/tmp/snpeexample on the Android target.</p>
<pre class="fragment">export SNPE_TARGET_ARCH=arm-android-clang6.0
export SNPE_TARGET_STL=libc++_shared.so

adb shell "mkdir -p /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin"
adb shell "mkdir -p /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib"
adb shell "mkdir -p /data/local/tmp/snpeexample/dsp/lib"

adb push $SNPE_ROOT/lib/$SNPE_TARGET_ARCH/$SNPE_TARGET_STL \
      /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib
adb push $SNPE_ROOT/lib/$SNPE_TARGET_ARCH/*.so \
      /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib
adb push $SNPE_ROOT/lib/dsp/*.so \
      /data/local/tmp/snpeexample/dsp/lib
adb push $SNPE_ROOT/bin/$SNPE_TARGET_ARCH/snpe-net-run \
      /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin
</pre><p><b>Set up enviroment variables</b></p>
<p>Set up the library path, the path variable, and the target architecture in adb shell to run the executable with the -h argument to see its description.</p>
<pre class="fragment">adb shell
export SNPE_TARGET_ARCH=arm-android-clang6.0
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib
export PATH=$PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin
snpe-net-run -h
exit
</pre><p><b>Push model data to Android target</b></p>
<p>To execute the Word-RNN model on your Android target follow these steps:</p>
<pre class="fragment">adb shell "mkdir -p /data/local/tmp/word_rnn"
adb push input_list.txt /data/local/tmp/word_rnn
adb push word_rnn.dlc /data/local/tmp/word_rnn
adb push word_rnn_adb.sh /data/local/tmp/word_rnn
</pre><p><b>Note:</b> It may take some time to push the word_rnn dlc file to your target.</p>
<h1><a class="anchor" id="tutorial_word_rnn_run_on_android_cpu"></a>
Running on Android using CPU Runtime</h1>
<p>Run the Android C++ executable with the following commands:</p>
<pre class="fragment">python3 inference.py --use_cpu
</pre><p>We will get the same result as when we <a class="el" href="tutorial_word_rnn.html#tutorial_word_rnn_run_on_linux_host">Run on Linux Host</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 -->
<!-- start footer part -->
<div id="nav-path" class="navpath" font-size:small;><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">
      <p align="right">
        80-NL315-14 A <br>
        MAY CONTAIN U.S. AND INTERNATIONAL EXPORT CONTROLLED INFORMATION
        <!--If the Controlled Distribution statement is to be included, uncomment below:-->
        <!--<b>Controlled Distribution - DO NOT COPY</b>-->
        <img class="footer" width:5%; alt="QTI Logo" src="images/QTI_Logo.png" />
      </p>
    </li>
  </ul>
</div>
</body>
</html>
