<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-->
<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"></meta>
<meta http-equiv="X-UA-Compatible" content="IE=9"></meta>
<title>Snapdragon Neural Processing Engine SDK: Running the Spoken Digit Recognition Model</title>
<link href="tabs.css" rel="stylesheet" type="text/css"></link>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="autoEnterCurrentDate.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="is.css" rel="stylesheet" type="text/css" ></link>
<link href="custom.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Snapdragon Neural Processing Engine SDK
   <span id="projectnumber"></span></div>
   <div id="projectbrief">Reference Guide</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('tutorial_spoken_digit.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Running the Spoken Digit Recognition Model </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="tutorial_spoken_digit_table_of_content"></a>
Table Of Contents</h1>
<ul>
<li><a class="el" href="tutorial_spoken_digit.html#tutorial_spoken_digit_overview_native_app">Overview</a></li>
<li><a class="el" href="tutorial_spoken_digit.html#tutorial_spoken_digit_overview">Introduction</a></li>
<li><a class="el" href="tutorial_spoken_digit.html#tutorial_spoken_digit_prerequisites">Prerequisites</a></li>
<li><a class="el" href="tutorial_spoken_digit.html#tutorial_spoken_digit_create_train_convert">Create, Train, and Convert Spoken Digit Model</a></li>
<li><a class="el" href="tutorial_spoken_digit.html#tutorial_spoken_digit_run_on_linux_host">Run on Linux Host</a></li>
<li><a class="el" href="tutorial_spoken_digit.html#tutorial_spoken_digit_run_on_android">Run on Android Target</a></li>
<li><a class="el" href="tutorial_spoken_digit.html#tutorial_spoken_digit_run_on_android_cpu">Running on Android using CPU Runtime</a></li>
</ul>
<h1><a class="anchor" id="tutorial_spoken_digit_overview_native_app"></a>
Overview</h1>
<p>The example C++ application in this tutorial is called <b>snpe-net-run</b>. It is a command line executable that executes a neural network using SNPE SDK APIs.</p>
<p>The required arguments to snpe-net-run are:</p><ul>
<li>A neural network model in the DLC file format</li>
<li>An input list file with paths to the input data.</li>
</ul>
<p>Optional arguments to snpe-net-run are:</p><ul>
<li>Choice of GPU or DSP runtime (default is CPU)</li>
<li>Output directory (default is ./output)</li>
<li>Show help description</li>
</ul>
<p>snpe-net-run creates and populates an output directory with the results of executing the neural network on the input data.</p>
<p><a class="anchor" id="fig_neural_network"></a>  <div style= text-align:left;><img src="images/neural_network.png" alt="SNPE" width="40%" height="40%"><br/><br/><b></b></div><br/> </p>
<p>The SNPE SDK provides Linux and Android binaries of <b>snpe-net-run</b> under</p><ul>
<li>$SNPE_ROOT/bin/x86_64-linux-clang</li>
<li>$SNPE_ROOT/bin/arm-android-clang6.0</li>
<li>$SNPE_ROOT/bin/aarch64-android-clang6.0</li>
<li>$SNPE_ROOT/bin/aarch64-oe-linux-gcc6.4</li>
<li>$SNPE_ROOT/bin/arm-oe-linux-gcc6.4hf</li>
</ul>
<h1><a class="anchor" id="tutorial_spoken_digit_overview"></a>
Introduction</h1>
<p>This chapter will show an example of recognizing the 10 classes in the free spoken digit dataset, with data processing and a 4-layer neural network, through SNPE. The step-by-step example will create, train, convert, and execute a TensorFlow-Keras audio model with SNPE.</p>
<p>As a prerequisite, users should download the <b>Free Spoken Digit Dataset (FSDD)</b>.</p>
<pre class="fragment">cd $SNPE_ROOT/models/spoken_digit
git clone https://github.com/Jakobovski/free-spoken-digit-dataset
</pre><p>The external required python3 packages to this example are:</p><ul>
<li>librosa</li>
<li>numpy</li>
<li>tensorflow (1.6 or 1.11)</li>
<li>tflearn</li>
</ul>
<p>There are five files and a single directory in the $SNPE_ROOT/models/spoken_digit folder</p><ul>
<li>free-spoken-digit-dataset (download from git)</li>
<li>input_list.txt</li>
<li>interpretRawDNNOutput.py</li>
<li>processSpokenDigitInput.py</li>
<li>spoken_digit.py</li>
<li>NOTICE.txt</li>
</ul>
<p>The <b>interpretRawDNNOutput.py</b> will translate SNPE output and display the prediction.</p>
<p>The <b>processSpokenDigitInput.py</b> processes user input wav audio file into raw format for snpe-net-run.</p>
<p>The <b>spoken_digit.py</b> python3 script creates and trains a 4-layer neural network model. After training is done, the corresponding frozen protobuf file will be generated.</p>
<p>The <b>free-spoken-digit-dataset</b> directory is the dataset downloaded by the user.</p>
<h1><a class="anchor" id="tutorial_spoken_digit_prerequisites"></a>
Prerequisites</h1>
<ul>
<li>
The SNPE SDK has been set up following the <a class="el" href="setup.html">SNPE Setup</a> chapter. </li>
<li>
The <a class="el" href="tutorial_setup.html">Tutorials Setup</a> has been completed. </li>
<li>
TensorFlow is installed (see <a class="el" href="setup_tensorflow.html">TensorFlow Setup</a>) </li>
</ul>
<h1><a class="anchor" id="tutorial_spoken_digit_create_train_convert"></a>
Create, Train, and Convert Spoken Digit Model</h1>
<p>Run spoken_digit.py to create and train the Word-RNN model.</p>
<pre class="fragment">python3 spoken_digit.py
</pre><p>The terminal will show the following messages.</p>
<pre class="fragment">Successfully split free-spoken-digit-dataset training/testing data.
Training data created.
---------------------------------
Run id: VNF8T3
Log directory: /tmp/tflearn_logs/
---------------------------------
Training samples: 512
Validation samples: 512
--
Training Step: 4  | total loss: 2.27042 | time: 1.091s
| Adam | epoch: 001 | loss: 2.27042 | val_loss: 2.32398 -- iter: 512/512
--
Training Step: 8  | total loss: 2.14184 | time: 1.013s
| Adam | epoch: 002 | loss: 2.14184 | val_loss: 2.49023 -- iter: 512/512
--
Training Step: 12  | total loss: 1.99593 | time: 1.049s
| Adam | epoch: 003 | loss: 1.99593 | val_loss: 2.66090 -- iter: 512/512
--
...
...
...
--
Training Step: 72  | total loss: 1.19825 | time: 1.047s
| Adam | epoch: 018 | loss: 1.19825 | val_loss: 5.10993 -- iter: 512/512
--
Training Step: 76  | total loss: 1.27894 | time: 1.019s
| Adam | epoch: 019 | loss: 1.27894 | val_loss: 5.20582 -- iter: 512/512
--
Training Step: 80  | total loss: 1.32618 | time: 1.014s
| Adam | epoch: 020 | loss: 1.32618 | val_loss: 5.38491 -- iter: 512/512
--
Optimization done.
Save frozen graph in spoken_digit.pb.
</pre><p>Next, convert the frozen graph model with snpe-tensorflow-to-dlc.</p>
<pre class="fragment">snpe-tensorflow-to-dlc --input_network spoken_digit.pb \
                       --input_dim InputData/X "1, 20, 35" \
                       --out_node "FullyConnected_3/Softmax" \
                       --output_path spoken_digit.dlc
</pre><p>After DLC conversion, we can view the converted dlc architecture with <b>snpe-dlc-info</b> and <b>snpe-dlc-viewer</b> as follows:</p>
<pre class="fragment">snpe-dlc-info -i spoken_digit.dlc
</pre><p>The output will be:</p>
<pre class="fragment">----------------------------------------------------------------------------------------------------------------------------------------------------------------------
| Id | Name                     | Type            | Inputs                     | Outputs                    | Out Dims | Runtimes | Parameters                       |
----------------------------------------------------------------------------------------------------------------------------------------------------------------------
| 0  | InputData/X:0            | data            | InputData/X:0              | InputData/X:0              | 1x20x35  | A D G C  | input_preprocessing: passthrough |
|    |                          |                 |                            |                            |          |          | input_type: default              |
| 1  | FullyConnected/MatMul    | fully_connected | InputData/X:0              | FullyConnected/BiasAdd:0   | 1x256    | A D G C  | param count: 179k (81.2%)        |
|    |                          |                 |                            |                            |          |          | MACs per inference: 179k (81.2%) |
| 2  | FullyConnected/Relu      | neuron          | FullyConnected/BiasAdd:0   | FullyConnected/Relu:0      | 1x256    | A D G C  | a: 0                             |
|    |                          |                 |                            |                            |          |          | b: 0                             |
|    |                          |                 |                            |                            |          |          | min_clamp: 0                     |
|    |                          |                 |                            |                            |          |          | max_clamp: 0                     |
|    |                          |                 |                            |                            |          |          | func: relu                       |
| 3  | FullyConnected_1/MatMul  | fully_connected | FullyConnected/Relu:0      | FullyConnected_1/BiasAdd:0 | 1x128    | A D G C  | param count: 32k (14.8%)         |
|    |                          |                 |                            |                            |          |          | MACs per inference: 32k (14.8%)  |
| 4  | FullyConnected_1/Relu    | neuron          | FullyConnected_1/BiasAdd:0 | FullyConnected_1/Relu:0    | 1x128    | A D G C  | a: 0                             |
|    |                          |                 |                            |                            |          |          | b: 0                             |
|    |                          |                 |                            |                            |          |          | min_clamp: 0                     |
|    |                          |                 |                            |                            |          |          | max_clamp: 0                     |
|    |                          |                 |                            |                            |          |          | func: relu                       |
| 5  | FullyConnected_2/MatMul  | fully_connected | FullyConnected_1/Relu:0    | FullyConnected_2/BiasAdd:0 | 1x64     | A D G C  | param count: 8k (3.71%)          |
|    |                          |                 |                            |                            |          |          | MACs per inference: 8k (3.71%)   |
| 6  | FullyConnected_2/Relu    | neuron          | FullyConnected_2/BiasAdd:0 | FullyConnected_2/Relu:0    | 1x64     | A D G C  | a: 0                             |
|    |                          |                 |                            |                            |          |          | b: 0                             |
|    |                          |                 |                            |                            |          |          | min_clamp: 0                     |
|    |                          |                 |                            |                            |          |          | max_clamp: 0                     |
|    |                          |                 |                            |                            |          |          | func: relu                       |
| 7  | FullyConnected_3/MatMul  | fully_connected | FullyConnected_2/Relu:0    | FullyConnected_3/BiasAdd:0 | 1x10     | A D G C  | param count: 641 (0.29%)         |
|    |                          |                 |                            |                            |          |          | MACs per inference: 640 (0.29%)  |
| 8  | FullyConnected_3/Softmax | softmax         | FullyConnected_3/BiasAdd:0 | FullyConnected_3/Softmax:0 | 1x10     | A D G C  |                                  |
----------------------------------------------------------------------------------------------------------------------------------------------------------------------
</pre><pre class="fragment">snpe-dlc-viewer -i spoken_digit.dlc
</pre><p>The output network model HTML file will be saved at /tmp/spoken_digit.html.</p>
<h1><a class="anchor" id="tutorial_spoken_digit_run_on_linux_host"></a>
Run on Linux Host</h1>
<p>First, <b>processSpokenDigitInput.py</b> script needs to be run in order to process the audio input data <b>test/5_jackson_0.wav</b> to raw format. The output name will be <b>input.raw</b></p>
<pre class="fragment">python3 processSpokenDigitInput.py test/5_jackson_0.wav
</pre><p>Next, run <b>snpe-net-run</b> to get the inference result.</p>
<pre class="fragment">snpe-net-run --container spoken_digit.dlc --input_list input_list.txt
</pre><p>After snpe-net-run completes, verify that the results are populated in the $SNPE_ROOT/models/spoken_digit/output directory. There should be one or more .log files and several Result_X directories.</p>
<p>The raw output prediction will be located in $SNPE_ROOT/models/spoken_digit/output/Result_0/FullyConnected_3/Softmax:0.raw. It holds the output tensor data of 10 probabilities for the 10 categories. The element with the highest value represents the top classification. We can use a python3 script to interpret the classification results as follows:</p>
<pre class="fragment">python3 interpretRawDNNOutput.py output/Result_0/FullyConnected_3/Softmax:0.raw
</pre><p>The output should look like the following, showing classification results for all the images.</p>
<pre class="fragment"> 0 : 0.000435
 1 : 0.017081
 2 : 0.000016
 3 : 0.001133
 4 : 0.000355
 5 : 0.692124
 6 : 0.006131
 7 : 0.039124
 8 : 0.002453
 9 : 0.241148
Classification Result: Class 5.
</pre><p>The final output shows the audio file was classified as "Class 5" (from a total of 10 labels) with a probability of 0.692124. Look at the rest of the output to see the model's classification on other classes.</p>
<p><b>Binary data input</b></p>
<p>Note that the spoken digit classification model does not accept wav files as input. The model expects its input tensor dimension to be <b>1 x 20 x 35</b> as a float array. The processSpokenDigitInput.py script performs a wav to binary data conversion. The script is an example of how wav audio files can be preprocessed to generate input for the classification model.</p>
<h1><a class="anchor" id="tutorial_spoken_digit_run_on_android"></a>
Run on Android Target</h1>
<p><b>Select target architecture</b></p>
<p>SNPE provides Android binaries for armeabi-v7a and arm64-v8a architectures. For each architecture, the binaries are compiled with clang6.0 using libc++ STL implementation. The following shows the commands to select the desired binaries.</p>
<pre class="fragment"># architecture: armeabi-v7a - compiler: clang - STL: libc++
export SNPE_TARGET_ARCH=arm-android-clang6.0
export SNPE_TARGET_STL=libc++_shared.so

# architecture: arm64-v8a - compiler: clang - STL: libc++
export SNPE_TARGET_ARCH=aarch64-android-clang6.0
export SNPE_TARGET_STL=libc++_shared.so
</pre><p>For simplicity, this tutorial sets the target binaries to arm-android-clang6.0, which use libc++_shared.so, for commands on host and on target.</p>
<p><b>Push binaries to target</b></p>
<p>Push SNPE libraries and the prebuilt snpe-net-run executable to /data/local/tmp/snpeexample on the Android target.</p>
<pre class="fragment">export SNPE_TARGET_ARCH=arm-android-clang6.0
export SNPE_TARGET_STL=libc++_shared.so

adb shell "mkdir -p /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin"
adb shell "mkdir -p /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib"
adb shell "mkdir -p /data/local/tmp/snpeexample/dsp/lib"

adb push $SNPE_ROOT/lib/$SNPE_TARGET_ARCH/$SNPE_TARGET_STL \
      /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib
adb push $SNPE_ROOT/lib/$SNPE_TARGET_ARCH/*.so \
      /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib
adb push $SNPE_ROOT/lib/dsp/*.so \
      /data/local/tmp/snpeexample/dsp/lib
adb push $SNPE_ROOT/bin/$SNPE_TARGET_ARCH/snpe-net-run \
      /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin
</pre><p><b>Set up enviroment variables</b></p>
<p>Set up the library path, the path variable, and the target architecture in adb shell to run the executable with the -h argument to see its description.</p>
<pre class="fragment">adb shell
export SNPE_TARGET_ARCH=arm-android-clang6.0
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib
export PATH=$PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin
snpe-net-run -h
exit
</pre><p><b>Push model data to Android target</b></p>
<p>To execute the spoken digit classification model on your Android target follow these steps:</p>
<pre class="fragment">adb shell "mkdir -p /data/local/tmp/spoken_digit"
adb push input.raw /data/local/tmp/spoken_digit
adb push input_list.txt /data/local/tmp/spoken_digit
adb push spoken_digit.dlc /data/local/tmp/spoken_digit
</pre><p><b>Note:</b> It may take some time to push the DLC file to your target.</p>
<h1><a class="anchor" id="tutorial_spoken_digit_run_on_android_cpu"></a>
Running on Android using CPU Runtime</h1>
<p>Run the Android C++ executable with the following commands:</p>
<pre class="fragment">adb shell
export SNPE_TARGET_ARCH=arm-android-clang6.0
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib
export PATH=$PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin
cd /data/local/tmp/spoken_digit
snpe-net-run --container spoken_digit.dlc --input_list input_list.txt
exit
</pre><p>The executable will create the results folder: /data/local/tmp/spoken_digit/output. To pull the output:</p>
<pre class="fragment">adb pull /data/local/tmp/spoken_digit/output output_android
</pre><p>Check the classification results by running the interpret python3 script.</p>
<pre class="fragment">python3 interpretRawDNNOutput.py output_android/Result_0/FullyConnected_3/Softmax:0.raw
</pre><p>The output should look like the following, showing classification results for all the images.</p>
<pre class="fragment"> 0 : 0.000435
 1 : 0.017081
 2 : 0.000016
 3 : 0.001133
 4 : 0.000355
 5 : 0.692124
 6 : 0.006131
 7 : 0.039124
 8 : 0.002453
 9 : 0.241148
Classification Result: Class 5.
</pre><h1><a class="anchor" id="tutorial_spoken_digit_run_on_android_gpu"></a>
Running on Android using GPU Runtime</h1>
<p>Try running on an Android target with the <b>--use_gpu</b> option as follows. By default, the GPU runtime runs in GPU_FLOAT32_16_HYBRID (math: full float and data storage: half float) mode. We can change the mode to GPU_FLOAT16 (math: half float and data storage: half float) using <b>&ndash;gpu_mode</b> option.</p>
<pre class="fragment">adb shell
export SNPE_TARGET_ARCH=arm-android-clang6.0
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib
export PATH=$PATH:/data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin
cd /data/local/tmp/spoken_digit
snpe-net-run --container spoken_digit.dlc --input_list input_list.txt --use_gpu
exit
</pre><p>Pull the output into an output_android_gpu directory.</p>
<pre class="fragment">adb pull /data/local/tmp/spoken_digit/output output_android_gpu
</pre><p>Again, we can run the interpret script to see the classification results.</p>
<pre class="fragment">python3 interpretRawDNNOutput.py output_android_gpu/Result_0/FullyConnected_3/Softmax:0.raw
</pre><p>The output should look like the following, showing classification results for all the images.</p>
<pre class="fragment"> 0 : 0.000440
 1 : 0.017181
 2 : 0.000017
 3 : 0.001144
 4 : 0.000360
 5 : 0.691406
 6 : 0.006180
 7 : 0.039307
 8 : 0.002481
 9 : 0.241211
Classification Result: Class 5.
</pre><p>Review the output for the classification results.</p>
<p>Classification results are identical to the run with CPU runtime, but there are differences in the probabilities associated with the output labels due to floating point precision differences. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 -->
<!-- start footer part -->
<div id="nav-path" class="navpath" font-size:small;><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">
      <p align="right">
        80-NL315-14 A <br>
        MAY CONTAIN U.S. AND INTERNATIONAL EXPORT CONTROLLED INFORMATION
        <!--If the Controlled Distribution statement is to be included, uncomment below:-->
        <!--<b>Controlled Distribution - DO NOT COPY</b>-->
        <img class="footer" width:5%; alt="QTI Logo" src="images/QTI_Logo.png" />
      </p>
    </li>
  </ul>
</div>
</body>
</html>
